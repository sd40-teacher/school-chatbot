import streamlit as st
from rag_engine import SchoolChatbot
from tts_engine import text_to_speech, get_audio_base64
import os
import base64

# ============================================================
# ğŸ”§ 1. ì•± ì„¤ì •
# ============================================================
VRM_MODEL_URL = "https://raw.githubusercontent.com/sd40-teacher/school-chatbot/main/sdg1.vrm"

st.set_page_config(page_title="ì„±ê¸€ê³  AI ë„ìš°ë¯¸", page_icon="ğŸ«", layout="wide")

try:
    api_key = st.secrets["OPENROUTER_API_KEY"]
except:
    st.error("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def load_chatbot():
    try:
        return SchoolChatbot(api_key=api_key, docs_path="data/school_docs")
    except Exception as e:
        return None

chatbot = load_chatbot()

# ============================================================
# ğŸ”§ 2. ì•„ë°”íƒ€ & ì˜¤ë””ì˜¤ í†µí•© ë·°ì–´ (í† ê¸€ + ìë™ì¬ìƒ ê¸°ëŠ¥ ì¶”ê°€)
# ============================================================
def vrm_viewer_component(audio_base64=None, auto_play=False):
    # ìë™ì¬ìƒ ëª¨ë“œì¼ ë•Œ ì˜¤ë””ì˜¤ ì´ˆê¸°í™” + ìë™ì¬ìƒ ì‹œë„
    audio_init_js = ""
    if audio_base64:
        if auto_play:
            # ìë™ì¬ìƒ ëª¨ë“œ: ë¡œë“œ í›„ ë°”ë¡œ ì¬ìƒ ì‹œë„
            audio_init_js = f"""
                const audio = document.getElementById("vrm-audio");
                audio.src = "data:audio/mp3;base64,{audio_base64}";
                const btn = document.getElementById("play-btn");
                btn.style.background = "#ff4b4b";
                btn.innerText = "ğŸ’¬ ë‹µë³€ ì¤‘...";
                
                // ìë™ì¬ìƒ ì‹œë„ (ì‚¬ìš©ì ìƒí˜¸ì‘ìš© í›„ì—ë§Œ ë™ì‘)
                audio.play().catch(e => {{
                    btn.innerText = "â–¶ ë‹µë³€ ë“£ê¸° (í´ë¦­)";
                }});
            """
        else:
            # ìˆ˜ë™ ëª¨ë“œ: ë²„íŠ¼ í´ë¦­ ëŒ€ê¸°
            audio_init_js = f"""
                const audio = document.getElementById("vrm-audio");
                audio.src = "data:audio/mp3;base64,{audio_base64}";
                const btn = document.getElementById("play-btn");
                btn.style.background = "#ff4b4b";
                btn.innerText = "â–¶ ë‹µë³€ ë“£ê¸° (í´ë¦­)";
            """

    html_code = f"""
    <div style="width: 100%; height: 620px; background: #8a94c8; border-radius: 20px; position: relative; overflow: hidden; display: flex; flex-direction: column;">
        <canvas id="vrm-canvas" style="width: 100%; height: 500px; cursor: grab;"></canvas>
        <audio id="vrm-audio" style="display:none;"></audio>
        
        <div style="height: 120px; background: #667eea; display: flex; flex-direction: column; justify-content: center; align-items: center; gap: 10px; padding: 10px;">
            <button id="play-btn" style="
                padding: 12px 30px; font-size: 16px; font-weight: bold; cursor: pointer; 
                background: #4CAF50; color: white; border: none; border-radius: 15px; 
                box-shadow: 0 4px 15px rgba(0,0,0,0.3); width: 85%;">
                {"ğŸ”ˆ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”" if not audio_base64 else "â–¶ ë‹µë³€ ë“£ê¸° (í´ë¦­)"}
            </button>
        </div>

        <script type="importmap">
        {{
            "imports": {{
                "three": "https://cdn.jsdelivr.net/npm/three@0.158.0/build/three.module.js",
                "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.158.0/examples/jsm/",
                "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3.1.3/lib/three-vrm.module.min.js"
            }}
        }}
        </script>

        <script type="module">
            import * as THREE from "three";
            import {{ GLTFLoader }} from "three/addons/loaders/GLTFLoader.js";
            import {{ OrbitControls }} from "three/addons/controls/OrbitControls.js";
            import {{ VRMLoaderPlugin }} from "@pixiv/three-vrm";

            let vrm = null;
            const scene = new THREE.Scene();
            const canvas = document.getElementById("vrm-canvas");
            
            const camera = new THREE.PerspectiveCamera(30, canvas.clientWidth / canvas.clientHeight, 0.1, 100);
            camera.position.set(0, 1.1, 2.2); 

            const renderer = new THREE.WebGLRenderer({{ canvas: canvas, antialias: true, alpha: true }});
            renderer.setSize(canvas.clientWidth, canvas.clientHeight);
            renderer.setPixelRatio(window.devicePixelRatio);

            const controls = new OrbitControls(camera, renderer.domElement);
            controls.target.set(0, 1.1, 0); 
            controls.update();

            scene.add(new THREE.AmbientLight(0xffffff, 0.7));
            const light = new THREE.DirectionalLight(0xffffff, 1.0);
            light.position.set(1, 2, 3);
            scene.add(light);

            const loader = new GLTFLoader();
            loader.register((parser) => new VRMLoaderPlugin(parser));
            loader.load("{VRM_MODEL_URL}", (gltf) => {{
                vrm = gltf.userData.vrm;
                scene.add(vrm.scene);
                vrm.scene.rotation.y = Math.PI;
                {audio_init_js}
            }});

            const audio = document.getElementById("vrm-audio");
            const btn = document.getElementById("play-btn");
            
            btn.onclick = () => {{ 
                if(audio.src && audio.paused) {{ 
                    audio.currentTime = 0; 
                    audio.play(); 
                    btn.innerText = "ğŸ’¬ ë‹µë³€ ì¤‘..."; 
                }} 
            }};
            
            audio.onplay = () => {{ btn.innerText = "ğŸ’¬ ë‹µë³€ ì¤‘..."; }};
            audio.onended = () => {{ btn.innerText = "ğŸ”„ ë‹¤ì‹œ ë“£ê¸°"; }};

            const clock = new THREE.Clock();
            function animate() {{
                requestAnimationFrame(animate);
                const delta = clock.getDelta();
                const time = clock.elapsedTime;

                if (vrm) {{
                    const spine = vrm.humanoid.getNormalizedBoneNode('spine');
                    const neck = vrm.humanoid.getNormalizedBoneNode('neck');
                    const hips = vrm.humanoid.getNormalizedBoneNode('hips');

                    if(spine) spine.rotation.x = Math.sin(time * 1.5) * 0.02; 
                    if(neck) neck.rotation.y = Math.sin(time * 0.7) * 0.04; 
                    if(hips) hips.position.y = Math.sin(time * 1.5) * 0.002;

                    vrm.update(delta);

                    // ë¦½ì‹±í¬
                    if (!audio.paused && !audio.ended && vrm.expressionManager) {{
                        const s = (Math.sin(Date.now() * 0.015) + 1) * 0.4;
                        ["aa", "oh", "Fcl_MTH_A", "Fcl_MTH_O"].forEach(k => {{
                            try {{ vrm.expressionManager.setValue(k, s); }} catch(e) {{}}
                        }});
                    }} else if (vrm.expressionManager) {{
                        ["aa","Fcl_MTH_A"].forEach(k => {{ try {{ vrm.expressionManager.setValue(k, 0); }} catch(e) {{}} }});
                    }}
                }}
                renderer.render(scene, camera);
            }}
            animate();
        </script>
    </div>
    """
    st.components.v1.html(html_code, height=640)

# ============================================================
# ğŸ”§ 3. ë©”ì¸ í™”ë©´ UI êµ¬ì„±
# ============================================================
st.title("ğŸ« ì„±ê¸€ê³  AI ë„ìš°ë¯¸")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë´ ì£¼ì„¸ìš”. ğŸ˜Š"}]
if "current_audio" not in st.session_state:
    st.session_state.current_audio = None
if "auto_voice" not in st.session_state:
    st.session_state.auto_voice = False
if "user_interacted" not in st.session_state:
    st.session_state.user_interacted = False

col_chat, col_vrm = st.columns([3, 2])

with col_chat:
    # ìë™ ìŒì„± í† ê¸€ (ìƒë‹¨ì— ë°°ì¹˜)
    col_toggle, col_info = st.columns([1, 2])
    with col_toggle:
        auto_voice = st.toggle(
            "ğŸ”Š ìë™ ìŒì„±", 
            value=st.session_state.auto_voice,
            help="ì¼œë©´ ë‹µë³€ì´ ìë™ìœ¼ë¡œ ì¬ìƒë©ë‹ˆë‹¤"
        )
        # í† ê¸€ ìƒíƒœ ë³€ê²½ ì‹œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ìœ¼ë¡œ ì¸ì •
        if auto_voice != st.session_state.auto_voice:
            st.session_state.auto_voice = auto_voice
            if auto_voice:
                st.session_state.user_interacted = True
    
    with col_info:
        if st.session_state.auto_voice:
            if st.session_state.user_interacted:
                st.caption("âœ… ìë™ ì¬ìƒ í™œì„±í™”ë¨")
            else:
                st.caption("âš ï¸ í† ê¸€ì„ ë‹¤ì‹œ ì¼œì„œ í™œì„±í™”í•˜ì„¸ìš”")
        else:
            st.caption("ğŸ’¡ ìë™ ìŒì„±ì„ ì¼œë©´ ë‹µë³€ì´ ë°”ë¡œ ì¬ìƒë©ë‹ˆë‹¤")
    
    st.divider()
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]): 
            st.markdown(msg["content"])
    
    # ì±„íŒ… ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): 
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            if chatbot:
                with st.spinner("ìƒê° ì¤‘..."):
                    response = chatbot.ask(prompt)
                st.markdown(response)
                
                # TTS ìƒì„±
                audio_bytes = text_to_speech(response)
                audio_base64 = get_audio_base64(audio_bytes)
                
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.session_state.current_audio = audio_base64
                st.rerun()

with col_vrm:
    st.subheader("ğŸ­ AI ì•„ë°”íƒ€")
    # ìë™ì¬ìƒ ì¡°ê±´: í† ê¸€ ON + ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì™„ë£Œ
    should_auto_play = st.session_state.auto_voice and st.session_state.user_interacted
    vrm_viewer_component(st.session_state.current_audio, auto_play=should_auto_play)
